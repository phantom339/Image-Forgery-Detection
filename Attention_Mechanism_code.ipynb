{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG9qJA0wKWNw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Conv2D, GlobalAveragePooling2D, GlobalMaxPooling2D, Dense, multiply, add, Concatenate, Activation, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from PIL import Image, ImageChops, ImageEnhance\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "import pickle\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_ela_image(path, quality, save_path=None):\n",
        "    original_image = Image.open(path).convert('RGB')\n",
        "    resaved_file_name = 'resaved_image.jpg'\n",
        "    original_image.save(resaved_file_name, 'JPEG', quality=quality)\n",
        "    resaved_image = Image.open(resaved_file_name)\n",
        "    ela_image = ImageChops.difference(original_image, resaved_image)\n",
        "\n",
        "    extrema = ela_image.getextrema()\n",
        "    max_difference = max([pix[1] for pix in extrema])\n",
        "    if max_difference == 0:\n",
        "        max_difference = 1\n",
        "    scale = 255 / max_difference\n",
        "\n",
        "    ela_image = ImageEnhance.Brightness(ela_image).enhance(scale)\n",
        "\n",
        "    if save_path:\n",
        "        ela_image.save(save_path)\n",
        "\n",
        "    return ela_image\n",
        "\n",
        "def prepare_image(image_path, save_path=None):\n",
        "    image_size = (128,128)\n",
        "    ela_image = convert_to_ela_image(image_path, 90, save_path)\n",
        "    return np.array(ela_image.resize(image_size)) / 255.0"
      ],
      "metadata": {
        "id": "Eu73n7-aKXkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "Y = []\n",
        "\n",
        "path_authentic = '/content/drive/MyDrive/Casia-my dataset/Au-ela'\n",
        "path_tampered = '/content/drive/MyDrive/Casia-my dataset/Tp-ela'\n",
        "output_path_authentic = '/content/drive/MyDrive/Casia-my dataset/Au-ela'\n",
        "output_path_tampered = '/content/drive/MyDrive/Casia-my dataset/Tp-ela'\n",
        "\n",
        "os.makedirs(output_path_authentic, exist_ok=True)\n",
        "os.makedirs(output_path_tampered, exist_ok=True)\n",
        "\n",
        "for filename in tqdm(os.listdir(path_authentic), desc=\"Processing Authentic Images : \"):\n",
        "    full_path = os.path.join(path_authentic, filename)\n",
        "    save_path = os.path.join(output_path_authentic, filename)\n",
        "    X.append(prepare_image(full_path,save_path))\n",
        "    Y.append(1)  # Authentic image label\n",
        "\n",
        "for filename in tqdm(os.listdir(path_tampered), desc=\"Processing Tampered Images : \"):\n",
        "    full_path = os.path.join(path_tampered, filename)\n",
        "    save_path = os.path.join(output_path_tampered, filename)\n",
        "    X.append(prepare_image(full_path,save_path))\n",
        "    Y.append(0)  # Tampered image label\n",
        "\n",
        "X = np.array(X)\n",
        "#Y= np.array(Y)\n",
        "\n",
        "Y = to_categorical(Y, 2)\n",
        "\n",
        "print(f'X shape: {X.shape}')\n",
        "print(f'Y shape: {Y.shape}')"
      ],
      "metadata": {
        "id": "0B1jWvfGKno7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
        "#X = X.reshape(-1,1,1,1)\n",
        "print(len(X_train), len(y_train))\n",
        "print(len(X_val), len(y_val))"
      ],
      "metadata": {
        "id": "BjskGxEzK7KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention Block\n",
        "\n"
      ],
      "metadata": {
        "id": "9vljq8XXLJZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBAM(Layer):\n",
        "    def __init__(self, filters, ratio=8):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.filters = filters\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.shared_dense_one = Dense(input_shape[-1] // self.ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)\n",
        "        self.shared_dense_two = Dense(input_shape[-1], kernel_initializer='he_normal', use_bias=False)\n",
        "        self.conv = Conv2D(1, (7, 7), strides=1, padding='same', activation='sigmoid', kernel_initializer='he_normal', use_bias=False)\n",
        "\n",
        "    def call(self, input_tensor):\n",
        "        # Channel Attention\n",
        "        avg_pool = GlobalAveragePooling2D()(input_tensor)\n",
        "        max_pool = GlobalMaxPooling2D()(input_tensor)\n",
        "\n",
        "        # Correct Reshape to keep the number of elements the same\n",
        "        avg_pool = Reshape((1, 1, avg_pool.shape[-1]))(avg_pool)\n",
        "        max_pool = Reshape((1, 1, max_pool.shape[-1]))(max_pool)\n",
        "\n",
        "        avg_out = self.shared_dense_two(self.shared_dense_one(avg_pool))\n",
        "        max_out = self.shared_dense_two(self.shared_dense_one(max_pool))\n",
        "\n",
        "        channel_attention = Activation('sigmoid')(add([avg_out, max_out]))\n",
        "        channel_attention = multiply([input_tensor, channel_attention])\n",
        "\n",
        "        # Spatial Attention\n",
        "        avg_pool = tf.reduce_mean(channel_attention, axis=-1, keepdims=True)\n",
        "        max_pool = tf.reduce_max(channel_attention, axis=-1, keepdims=True)\n",
        "        spatial_attention = Concatenate(axis=-1)([avg_pool, max_pool])\n",
        "        spatial_attention = self.conv(spatial_attention)\n",
        "\n",
        "        spatial_attention = multiply([channel_attention, spatial_attention])\n",
        "        return spatial_attention\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "metadata": {
        "id": "B_s3zqZ8LOhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_schedule(epoch, lr):\n",
        "    init_lr = .0007\n",
        "    total_epochs=70\n",
        "    decay = init_lr / total_epochs\n",
        "    new_lr = init_lr * (1 / (1 + decay * (epoch + 30)))\n",
        "\n",
        "    return max(new_lr, 1e-5)"
      ],
      "metadata": {
        "id": "AF0GeqGoLU_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_with_cbam(input_shape=(128,128,3)):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional Block 1\n",
        "    conv1 = Conv2D(32, (2, 2), activation='relu',padding = 'valid')(input_layer)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    # CNN block 2\n",
        "    conv2 = Conv2D(64, (2, 2), activation='relu',padding = 'valid')(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    #CNN block3\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu')(pool2)\n",
        "    conv4 = Conv2D(128, (3, 3), activation='relu')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    #CNN block4\n",
        "    conv5 = Conv2D(256, (3, 3), activation='relu')(pool3)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu',padding='same')(conv5)\n",
        "\n",
        "\n",
        "    # CBAM Block 2\n",
        "    cbam1 = CBAM(filters=32)(conv6)\n",
        "\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    gap = GlobalAveragePooling2D()(cbam1)\n",
        "\n",
        "    dense1 = Dense(64, activation='relu')(gap)\n",
        "    dropout = Dropout(0.4)(dense1)\n",
        "    dense2 = Dense(128, activation='relu')(dropout)\n",
        "    dropout = Dropout(0.4)(dense2)\n",
        "    output_layer = Dense(2, activation='softmax')(dropout)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0007), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = create_model_with_cbam()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "PRVBCRzgOLl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=32,callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "dah-0HoFMJpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "oWPMbeVVMnE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vkYqBuoqM2A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FRgcRRvFM7DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "NiU5gYddNewB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print(f'Validation loss: {loss:.4f}, accuracy: {accuracy:.4f}')\n",
        "\n",
        "y_pred_prob = model.predict(X_val)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int).flatten()"
      ],
      "metadata": {
        "id": "m065epawNw-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(X_val)\n",
        "# Convert predictions classes to one hot vectors\n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1)\n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(y_val,axis = 1)\n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
        "# plot the confusion matrix\n",
        "plot_confusion_matrix(confusion_mtx, classes = range(2))"
      ],
      "metadata": {
        "id": "aPOczdKhNsxR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}